{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping NBC news wild fires with Selenium and Beautifulsoup\n",
    "def get_list_link_from_subject(subject, num_pages):\n",
    "\n",
    "\n",
    "    PATH = \"./chromedriver_win32/chromedriver.exe\"\n",
    "    driver = webdriver.Chrome(PATH)\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    driver.get(\"https://www.google.com/search?q=\"+subject+\"&rlz=1C1CHBF_frFR863FR863&biw=1920&bih=880&sxsrf=AOaemvI0XcPZB9YWw9GUVGwWTEXPDVqRxQ:1638967714934&source=lnms&tbm=nws&sa=X&ved=2ahUKEwjGlsnDntT0AhWTTcAKHeyuDk4Q_AUoAXoECAEQAw\")\n",
    "\n",
    "    driver.find_element(By.XPATH, \"//button[@class='VfPpkd-LgbsSe VfPpkd-LgbsSe-OWXEXe-k8QpJ VfPpkd-LgbsSe-OWXEXe-dgl2Hf nCP5yc AjY5Oe DuMIQc']\").click() #accept google policy\n",
    "\n",
    "    for i in range(num_pages):\n",
    "        if i != 0:\n",
    "            driver.find_element(By.ID, \"pnnext\").click()\n",
    "\n",
    "        html_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html_source, 'lxml')\n",
    "\n",
    "        #Getting all g-card \n",
    "        g_card_list = soup.find_all(\"g-card\")\n",
    "\n",
    "        for g_card in g_card_list:\n",
    "            a = g_card.find(\"a\")\n",
    "            link = a['href']\n",
    "            link_list.append(link)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Successfully scraped : \", len(link_list), \" links\")\n",
    "\n",
    "    return link_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the link list, for each link try to scrape the article if there is one and the date if there is one.\n",
    "def get_df_from_link_list(link_list):\n",
    "\n",
    "    my_timeout = 10\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i, link in enumerate(link_list):\n",
    "        d = {}\n",
    "        print(i+1, \"/\", len(link_list))\n",
    "\n",
    "        try:\n",
    "            html_text = requests.get(link, timeout=my_timeout).text\n",
    "\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "            title = soup.find('title')\n",
    "            if title != None:\n",
    "                d[\"Title\"] = title.text\n",
    "\n",
    "            d[\"Link\"] = link\n",
    "   \n",
    "            time = soup.find('time')\n",
    "            if time != None:\n",
    "                if time.has_attr('datetime'):\n",
    "\n",
    "                    d[\"Date\"] = time['datetime']\n",
    "            \n",
    "            article = soup.find('article')\n",
    "            if article != None:\n",
    "                paragraphs = article.find_all('p')\n",
    "            big_p = \"\"\n",
    "            for p in paragraphs:\n",
    "                big_p = big_p + \" \" + p.text \n",
    "            \n",
    "            if big_p != \"\":\n",
    "                d[\"Content\"] = big_p\n",
    "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as err: #Requests takes way too long\n",
    "            print('Server taking too long. Try again later for page number' + str(i))\n",
    "\n",
    "        data.append(d)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_subject(subject,num_pages):\n",
    "    return get_df_from_link_list(get_list_link_from_subject(subject, num_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SORLUC~1\\AppData\\Local\\Temp/ipykernel_28448/477364689.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped :  49  links\n",
      "1 / 49\n",
      "yo1\n",
      "yo2\n",
      "yo3\n"
     ]
    }
   ],
   "source": [
    "my_scraped_data = get_df_from_subject(\"pokemon\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title      0\n",
       "Link       0\n",
       "Date       0\n",
       "Content    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews_df.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "877e635d77654b4bbaf40ac73f166c6702f150ac966450346b4e10ac33844dc6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
