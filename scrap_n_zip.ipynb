{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from django.http import HttpResponse\n",
    "\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping NBC news wild fires with Selenium and Beautifulsoup\n",
    "def get_list_link_from_subject(subject, num_pages):\n",
    "\n",
    "\n",
    "    PATH = \"./chromedriver_win32/chromedriver.exe\"\n",
    "\n",
    "    s=Service(PATH)\n",
    "    driver = webdriver.Chrome(service=s)\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    driver.get(\"https://www.google.com/search?q=\"+subject+\"&rlz=1C1CHBF_frFR863FR863&biw=1920&bih=880&sxsrf=AOaemvI0XcPZB9YWw9GUVGwWTEXPDVqRxQ:1638967714934&source=lnms&tbm=nws&sa=X&ved=2ahUKEwjGlsnDntT0AhWTTcAKHeyuDk4Q_AUoAXoECAEQAw\")\n",
    "\n",
    "    driver.find_element(By.XPATH, \"//button[@class='VfPpkd-LgbsSe VfPpkd-LgbsSe-OWXEXe-k8QpJ VfPpkd-LgbsSe-OWXEXe-dgl2Hf nCP5yc AjY5Oe DuMIQc']\").click() #accept google policy\n",
    "\n",
    "    for i in range(num_pages):\n",
    "        if i != 0:\n",
    "            driver.find_element(By.ID, \"pnnext\").click()\n",
    "\n",
    "        html_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(html_source, 'lxml')\n",
    "\n",
    "        #Getting all g-card \n",
    "        g_card_list = soup.find_all(\"g-card\")\n",
    "\n",
    "        for g_card in g_card_list:\n",
    "            a = g_card.find(\"a\")\n",
    "            link = a['href']\n",
    "            link_list.append(link)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"Successfully scraped : \", len(link_list), \" links\")\n",
    "\n",
    "    return link_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the link list, for each link try to scrape the article if there is one and the date if there is one.\n",
    "def get_df_from_link_list(link_list):\n",
    "\n",
    "    my_timeout = 10\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i, link in enumerate(link_list):\n",
    "        d = {}\n",
    "\n",
    "        try:\n",
    "            html_text = requests.get(link, timeout=my_timeout).text\n",
    "\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "            title = soup.find('title')\n",
    "            if title != None:\n",
    "                d[\"Title\"] = title.text\n",
    "\n",
    "            d[\"Link\"] = link\n",
    "   \n",
    "            time = soup.find('time')\n",
    "            if time != None:\n",
    "                if time.has_attr('datetime'):\n",
    "\n",
    "                    d[\"Date\"] = time['datetime']\n",
    "            \n",
    "            article = soup.find('article')\n",
    "            if article != None:\n",
    "                paragraphs = article.find_all('p')\n",
    "                big_p = \"\"\n",
    "                for p in paragraphs:\n",
    "                    big_p = big_p + p.text + \" \"\n",
    "                \n",
    "                if big_p != \"\":\n",
    "                    d[\"Content\"] = unicodedata.normalize(\"NFKD\", big_p).rstrip()\n",
    "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as err: #Requests takes way too long\n",
    "            print('Server taking too long. Try again later for page number' + str(i))\n",
    "\n",
    "        data.append(d)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_subject(subject,num_pages):\n",
    "    return get_df_from_link_list(get_list_link_from_subject(subject, num_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_of_date(df):\n",
    "    res = df.drop(columns = [\"Date\"])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_list_from_subject(subject,num_pages):\n",
    "    df = get_df_from_subject(subject,num_pages)\n",
    "    articles = df[\"Content\"]\n",
    "    articles = articles.dropna()\n",
    "    return list(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_list_of_articles(article_list, zip_file_name):\n",
    "    os.mkdir(\"temp\")\n",
    "    for i,lines in enumerate(article_list):\n",
    "        with open(\"temp/article_number_\" + str(i) + \".txt\",'w',encoding='utf-8') as f:\n",
    "            f.write(lines)\n",
    "    shutil.make_archive(zip_file_name, 'zip', \"temp\")\n",
    "    shutil.rmtree(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_articles_from_subject(subject,num_pages,zip_file_name):\n",
    "    article_list = get_article_list_from_subject(subject,num_pages)\n",
    "    zip_list_of_articles(article_list,zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped :  100  links\n",
      "Server taking too long. Try again later for page number12\n",
      "Server taking too long. Try again later for page number45\n"
     ]
    }
   ],
   "source": [
    "zip_articles_from_subject(\"Wildfire\",10,\"Wildfire_test\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "877e635d77654b4bbaf40ac73f166c6702f150ac966450346b4e10ac33844dc6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
