{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from streamlit_utils import *\n",
    "from datetime import datetime\n",
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "def tokenize_input_text(question, input_text):\n",
    "\n",
    "    input_ids = tokenizer.encode(question, input_text)\n",
    "\n",
    "    return input_ids\n",
    "def get_segment_ids(input_ids):\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    return segment_ids\n",
    "def get_scores(input_ids,segment_ids):\n",
    "    # Run through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids]),\n",
    "                                    return_dict = False) # The segment IDs to differentiate question from answer_text\n",
    "    return start_scores, end_scores\n",
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "def get_answer(start_scores,end_scores, input_ids):\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Combine the tokens in the answer and print it out.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "    return answer\n",
    "# Better version\n",
    "\n",
    "def get_answer_clean(start_scores, end_scores, input_ids):\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "    return answer\n",
    "def answer_from_question(question, input_text):\n",
    "    input_ids = tokenize_input_text(question, input_text)\n",
    "    segment_ids = get_segment_ids(input_ids)\n",
    "    start_scores, end_scores = get_scores(input_ids, segment_ids)\n",
    "\n",
    "    res = get_answer_clean(start_scores, end_scores, input_ids)\n",
    "\n",
    "    if '[CLS]' in res:\n",
    "        return \"No answer found\"    \n",
    "    return res\n",
    "def get_first_senteces_from_sentences_list(sentences_list, nb_sentences):\n",
    "    res = \"\".join(sentences_list[0:nb_sentences])\n",
    "    return res\n",
    "def add_QA_location_to_df(df_location, subject, nb_sentences=9):\n",
    "    question = \"Where did the \" + subject + \" occur?\"\n",
    "    df_location[\"QA_location\"] = None\n",
    "    df_location[\"QA_location\"] = df_location[\"Sentences\"].apply(lambda x : answer_from_question(question, get_first_senteces_from_sentences_list(x,nb_sentences)))\n",
    "    return df_location\n",
    "def add_QA_impact_to_df(df_location, subject, nb_sentences=10):\n",
    "    question = \"What were the impact of the \" + subject + \" ?\"\n",
    "    df_location[\"QA_impact\"] = None\n",
    "    df_location[\"QA_impact\"] = df_location[\"Sentences\"].apply(lambda x : answer_from_question(question, get_first_senteces_from_sentences_list(x,nb_sentences)))\n",
    "    return df_location\n",
    "def add_QA_cause_to_df(df_location, subject, nb_sentences=10):\n",
    "    question = \"What caused the \" + subject + \" ?\"\n",
    "    df_location[\"QA_cause\"] = None\n",
    "    df_location[\"QA_cause\"] = df_location[\"Sentences\"].apply(lambda x : answer_from_question(question, get_first_senteces_from_sentences_list(x,nb_sentences)))\n",
    "    return df_location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_QA(df_location_QA):\n",
    "    answer_list = df_location_QA[\"QA_location\"]\n",
    "    ner_list = df_location_QA[\"Location\"]\n",
    "    res = 0\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm') #Loading english NLP model\n",
    "            \n",
    "    score = 0\n",
    "\n",
    "    for i in range(len(ner_list)):\n",
    "        ner_loc = ner_list[i]\n",
    "        \n",
    "        \n",
    "        n = len(ner_loc)\n",
    "        for j in range(n):\n",
    "            doc = nlp(answer_list[i])\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'GPE':\n",
    "                    if ent.text == ner_loc[j].lower():\n",
    "                        \n",
    "                        score += 1\n",
    "\n",
    "    score = score/len(answer_list)\n",
    "    \n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped :  10  links\n",
      "Could not scrap page number 8, try again another time.\n",
      "Done scraping\n",
      "There are 5 usable articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>Clean_content</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Location</th>\n",
       "      <th>QA_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How climate change-driven wildfires are changi...</td>\n",
       "      <td>https://www.cnn.com/2020/08/24/weather/califor...</td>\n",
       "      <td>2020-08-24</td>\n",
       "      <td>By Ray Sanchez, Brandon Miller and Judson Jone...</td>\n",
       "      <td>[By Ray Sanchez, Brandon Miller and Judson Jon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>the golden state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California wildfires 2020: Why the current fir...</td>\n",
       "      <td>https://www.vox.com/2020/8/21/21377181/califor...</td>\n",
       "      <td>2020-08-26</td>\n",
       "      <td>Dry lightning, extreme heat, and Covid 19 are ...</td>\n",
       "      <td>[Dry lightning, extreme heat, and Covid 19 are...</td>\n",
       "      <td>[California, San Francisco, Santa Clara, Alame...</td>\n",
       "      <td>southern san francisco bay area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gender-reveal party culprit in massive Califor...</td>\n",
       "      <td>https://globalnews.ca/news/7320113/gender-reve...</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>A firework at a gender reveal party triggered ...</td>\n",
       "      <td>[A firework at a gender reveal party triggered...</td>\n",
       "      <td>[California, Yucaipa, Los Angeles, Mountain Ho...</td>\n",
       "      <td>southern california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gender reveal party sparks large Californian w...</td>\n",
       "      <td>https://www.abc.net.au/news/2020-09-08/gender-...</td>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>On Saturday morning an expectant couple trekke...</td>\n",
       "      <td>[On Saturday morning an expectant couple trekk...</td>\n",
       "      <td>[California, Yucaipa, US, Arizona, Dickey, Kno...</td>\n",
       "      <td>el ranch dorado park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia wildfires: Here's what you need to k...</td>\n",
       "      <td>https://www.cnn.com/2020/01/01/australia/austr...</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>By Jessie Yeung, CNN  Updated 0202 GMT  Januar...</td>\n",
       "      <td>[By Jessie Yeung, CNN  Updated 0202 GMT  Janua...</td>\n",
       "      <td>[Australia]</td>\n",
       "      <td>australia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  How climate change-driven wildfires are changi...   \n",
       "1  California wildfires 2020: Why the current fir...   \n",
       "2  Gender-reveal party culprit in massive Califor...   \n",
       "3  Gender reveal party sparks large Californian w...   \n",
       "4  Australia wildfires: Here's what you need to k...   \n",
       "\n",
       "                                                Link        Date  \\\n",
       "0  https://www.cnn.com/2020/08/24/weather/califor...  2020-08-24   \n",
       "1  https://www.vox.com/2020/8/21/21377181/califor...  2020-08-26   \n",
       "2  https://globalnews.ca/news/7320113/gender-reve...  2020-09-07   \n",
       "3  https://www.abc.net.au/news/2020-09-08/gender-...  2020-09-07   \n",
       "4  https://www.cnn.com/2020/01/01/australia/austr...  2020-01-13   \n",
       "\n",
       "                                       Clean_content  \\\n",
       "0  By Ray Sanchez, Brandon Miller and Judson Jone...   \n",
       "1  Dry lightning, extreme heat, and Covid 19 are ...   \n",
       "2  A firework at a gender reveal party triggered ...   \n",
       "3  On Saturday morning an expectant couple trekke...   \n",
       "4  By Jessie Yeung, CNN  Updated 0202 GMT  Januar...   \n",
       "\n",
       "                                           Sentences  \\\n",
       "0  [By Ray Sanchez, Brandon Miller and Judson Jon...   \n",
       "1  [Dry lightning, extreme heat, and Covid 19 are...   \n",
       "2  [A firework at a gender reveal party triggered...   \n",
       "3  [On Saturday morning an expectant couple trekk...   \n",
       "4  [By Jessie Yeung, CNN  Updated 0202 GMT  Janua...   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                                 []   \n",
       "1  [California, San Francisco, Santa Clara, Alame...   \n",
       "2  [California, Yucaipa, Los Angeles, Mountain Ho...   \n",
       "3  [California, Yucaipa, US, Arizona, Dickey, Kno...   \n",
       "4                                        [Australia]   \n",
       "\n",
       "                       QA_location  \n",
       "0                 the golden state  \n",
       "1  southern san francisco bay area  \n",
       "2              southern california  \n",
       "3             el ranch dorado park  \n",
       "4                        australia  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date = datetime(2020,1,1)\n",
    "max_date = datetime(2021,1,1)\n",
    "date_limits = (min_date, max_date)\n",
    "subject = \"wildfire\"\n",
    "nb_pages = 1\n",
    "\n",
    "\n",
    "df_location = get_locations_df_from_subject(subject, nb_pages, date_limits)\n",
    "\n",
    "\n",
    "nb_sentences = 9\n",
    "df_location = add_QA_location_to_df(df_location, subject, nb_sentences)\n",
    "df_location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indonesia west java\n",
      "indonesia sumatra\n",
      "indonesia indonesia\n",
      "indonesia jakarta\n",
      "indonesia rome\n",
      "indonesia atlanta\n",
      "indonesia georgia\n",
      "indonesia indonesia\n",
      "indonesia tarpaulin\n",
      "indonesia indonesia\n",
      "indonesia new zealand\n",
      "indonesia sulawesi\n",
      "indonesia palang\n",
      "cigondong jakarta\n",
      "jakarta jakarta\n",
      "indonesia jakarta\n",
      "cigondong indonesia\n",
      "jakarta indonesia\n",
      "indonesia indonesia\n",
      "cigondong jakarta\n",
      "jakarta jakarta\n",
      "indonesia jakarta\n",
      "cigondong the sunda strait\n",
      "jakarta the sunda strait\n",
      "indonesia the sunda strait\n",
      "cigondong japan\n",
      "jakarta japan\n",
      "indonesia japan\n",
      "cigondong krakatoa\n",
      "jakarta krakatoa\n",
      "indonesia krakatoa\n",
      "cigondong philippines\n",
      "jakarta philippines\n",
      "indonesia philippines\n",
      "cigondong the united states\n",
      "jakarta the united states\n",
      "indonesia the united states\n",
      "cigondong alaska\n",
      "jakarta alaska\n",
      "indonesia alaska\n",
      "cigondong hawaii\n",
      "jakarta hawaii\n",
      "indonesia hawaii\n",
      "cigondong washington\n",
      "jakarta washington\n",
      "indonesia washington\n",
      "cigondong cigondong\n",
      "jakarta cigondong\n",
      "indonesia cigondong\n",
      "cigondong krakatau\n",
      "jakarta krakatau\n",
      "indonesia krakatau\n",
      "cigondong tokyo\n",
      "jakarta tokyo\n",
      "indonesia tokyo\n",
      "cigondong manila\n",
      "jakarta manila\n",
      "indonesia manila\n",
      "indonesia indonesia\n",
      "indonesia sulawesi\n",
      "indonesia palu\n",
      "indonesia uk\n",
      "indonesia australia\n",
      "alaska california\n",
      "alaska vancouver island\n",
      "alaska british columbia\n",
      "alaska santa barbara\n",
      "alaska ventura\n",
      "alaska alaska\n",
      "alaska crescent city\n",
      "alaska canada\n",
      "alaska the united states\n",
      "alaska japan\n",
      "alaska santa cruz\n",
      "indonesia indonesia\n",
      "indonesia sulawesi\n",
      "indonesia bencana\n",
      "indonesia japan\n",
      "indonesia kobe\n",
      "indonesia germany\n",
      "indonesia palu city\n",
      "indonesia donggala\n",
      "tampa florida\n",
      "tampa tampa\n",
      "tampa apollo beach\n",
      "tampa advisory\n",
      "tampa u.s.\n",
      "tampa houston\n",
      "tampa hawaii\n",
      "alaska alaska\n",
      "alaska us\n",
      "alaska anchorage\n",
      "alaska kodiak\n",
      "alaska kenai\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_QA(df_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped :  100  links\n",
      "Could not scrap page number 23, try again another time.\n",
      "Could not scrap page number 46, try again another time.\n",
      "Done scraping\n",
      "There are 64 usable articles\n",
      "Successfully scraped :  100  links\n",
      "Could not scrap page number 2, try again another time.\n",
      "Could not scrap page number 3, try again another time.\n",
      "Done scraping\n",
      "There are 58 usable articles\n",
      "Successfully scraped :  100  links\n",
      "Could not scrap page number 26, try again another time.\n",
      "Could not scrap page number 42, try again another time.\n",
      "Could not scrap page number 93, try again another time.\n",
      "Done scraping\n",
      "There are 57 usable articles\n"
     ]
    }
   ],
   "source": [
    "subject_list = [\"wildfire\", \"tsunami\", \"earthquake\"]\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "min_date = datetime(2021,1,1)\n",
    "max_date = datetime(2022,1,1)\n",
    "date_limits = (min_date, max_date)\n",
    "nb_pages = 10\n",
    "nb_sentences = 5\n",
    "\n",
    "for subject in subject_list:\n",
    "    \n",
    "    df_location = get_locations_df_from_subject(subject, nb_pages, date_limits)\n",
    "   \n",
    "    df_location = add_QA_location_to_df(df_location, subject, nb_sentences)\n",
    "    success = True\n",
    "        \n",
    "\n",
    "    \n",
    "    score_dict[subject] = evaluate_QA(df_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wildfire': 0.515625,\n",
       " 'tsunami': 0.5344827586206896,\n",
       " 'earthquake': 0.42105263157894735}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "877e635d77654b4bbaf40ac73f166c6702f150ac966450346b4e10ac33844dc6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
